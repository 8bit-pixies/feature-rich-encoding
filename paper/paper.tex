\section{Feature-Rich Encoding for Uplift
Modelling}\label{feature-rich-encoding-for-uplift-modelling}

Chapman Siu

\section{Abstract}\label{abstract}

There have been many approaches for word embeddings which have been
explored including \texttt{word2vec} and \texttt{tfidf}.

In this paper we will explore whether enriching our data using
approaches using linguistic approaches such as POS and NER tagging will
provide substatial uplift to our models which was described as
``Feature-Rich Encoding''(Nallapati, Xiang, and Zhou 2016).

\emph{Keyword - Text Mining}

\section{Introduction}\label{introduction}

Text document classification is a task of classifying a document into
predefined categories based on the contents of the document. A document
is represented by a piece of text expressed as words or phrases. The
task of traditional text categorization methods is done by human
experts. It usually needs a large amount of time to deal with the lack
of text categorization. In recent years, text categorization has become
an important research topic in machine learning and information
retrieval. It has also become an important research topic in text
mining, which analyses and extracts useful information from texts. More
Learning techniques has been in research for dealing with text
categorization.

In recent years there have been lots of interest in various approaches
for word embeddings, which generally fall into the following categories:

\begin{itemize}
\tightlist
\item
  term frequency approaches, including term frequency-inverse document
  frequency approaches (TFIDF), and latent semantic analysis approaches
\item
  word2vec approaches including skip-gram and continuous bag of words
  approaches (cbow)
\item
  latent dirichlet allocation
\end{itemize}

\section{Preliminaries}\label{preliminaries}

\subsection{Latent Dirichlet
Allocation}\label{latent-dirichlet-allocation}

Latent Dirichlet Allocation (LDA) is a well-known topic modelling
technique proposed by (Blei, Ng, and Jordan 2003). LDA is a generative
probabilistic model of a textual corpus (i.e., a set of textual
documents), which takes a training textual corpus as input, and a number
of parameters including the number of topics \((K)\) considered. In the
training phase, for each document \(s\), LDA will compute its topic
distribution \(\theta_s\), which is a vector with \(K\) elements, and
each element corresponds to a topic. The value of each element in
\(\theta_s\) is a real number from \(0\) to \(1\), which represents the
proportion of the words in \(s\) that belong to the corresponding topic
in \(s\). After training, LDA can be used to predict the topic
distribution \(\theta_m\) of a new document \(m\). In our case, a
document is the description of a question, and the topic is a higher
level concept corresponding to a distribution of words. For example, we
may have the topic ``admissions'', which is a distribution of words such
as ``citizenship'', ``GRE'', ``TOEFL'', ``transcripts''.

\subsection{Word2Vec}\label{word2vec}

Word2Vec is all about computing distributed vector representations of
words. In this project we will be using the skip-gram variant.

The training objective of skip-gram is to learn word vector
representations that are good at predicting its context in the same
sentence. Mathematically, given a sequence of training words
\(w_1,w_2,...,w_T\), the objective of the skip-gram model is to maximize
the average log-likelihood

\[ \frac{1}{T}\sum_{t=1}^T\sum_{j=-k}^{j=k} \log \Pr(w_{t+j} | w_t) \]

where \(k\) is the size of the training window.

In the skip-gram model, every word \(w\) is associated with two vectors
\(u_w\) and \(v_w\) which are vector representations of \(w\) as word
and context respectively. The probability of correctly predicting word
\(w_i\) given word \(w_j\) is determined by the softmax model, which is

\[ \Pr (w_i | w_j) = \frac{\exp(u_{w_i}^T v_{w_j})}{\sum_{l=1}^V \exp(u_l^T v_{w_j}} \]

where \(V\) is the vocabulary size.

The skip-gram model with softmax is expensive because the cost of
computing \(\log (\Pr(w_i | w_j))\) is proportional to \(V\), which can
be easily in order of millions.

\subsection{Latent Semantic Indexing}\label{latent-semantic-indexing}

Latent Semantic indexing is a transformation on bag-of-words models by
applying truncated SVD to term-document matrices. This can be performed
on word counts or tf-idf (term frequency-inverse document frequency).

\section{Proposed Approach}\label{proposed-approach}

In this section we will present the overall framework for our
feature-rich encoder. We will consider the original one proposed by
(Nallapati, Xiang, and Zhou 2016) and extending it with other commonly
used word embeddings such as LDA.

\subsection{Overall Framework}\label{overall-framework}

The framework for this feature building consists of building the
relevant word representations. It consists of:

\begin{itemize}
\tightlist
\item
  word2vec
\item
  POS, with word2vec embedding
\item
  NER, with word2vec embedding
\item
  TFIDF
\item
  LDA
\end{itemize}

In order to build these features, various Python libraries were used,
including \texttt{scikit-learn}(Buitinck et al. 2013) in order to
combine all the features together, \texttt{gensim}(Rehurek and Sojka
2010) for the LDA and word2vec implementations, and \texttt{nltk}(Bird,
Klein, and Loper 2009) which is used to extract POS and NER information.

\subsection{Supervised Models}\label{supervised-models}

In order to compare uplift in performance, we will compare the uplift
gained from various popular models which are used in text classification
and machine learning including naive bayes(Aghila and others 2010),
svm(Joachims 1998), decision trees(Witschel, n.d.).

\section{Experiments and Results}\label{experiments-and-results}

\subsection{Experimental Setup}\label{experimental-setup}

The data sets used for this project was the 20 new groups
dataset(Joachims 1996) and various datasets from UCI

\subsection{Evaluation Metrics}\label{evaluation-metrics}

Accuracy was used as inline with results discussed in (Joachims 1996).

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-aghila2010survey}{}
Aghila, G, and others. 2010. ``A Survey of Na\(\backslash\)" Ive Bayes
Machine Learning Approach in Text Document Classification.'' \emph{ArXiv
Preprint ArXiv:1003.1795}.

\hypertarget{ref-bird2009natural}{}
Bird, Steven, Ewan Klein, and Edward Loper. 2009. \emph{Natural Language
Processing with Python}. `` O'Reilly Media, Inc.''

\hypertarget{ref-blei2003latent}{}
Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. ``Latent
Dirichlet Allocation.'' \emph{Journal of Machine Learning Research} 3
(Jan): 993--1022.

\hypertarget{ref-sklearn_api}{}
Buitinck, Lars, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa,
Andreas Mueller, Olivier Grisel, Vlad Niculae, et al. 2013. ``API Design
for Machine Learning Software: Experiences from the Scikit-Learn
Project.'' In \emph{ECML Pkdd Workshop: Languages for Data Mining and
Machine Learning}, 108--22.

\hypertarget{ref-joachims1996probabilistic}{}
Joachims, Thorsten. 1996. ``A Probabilistic Analysis of the Rocchio
Algorithm with Tfidf for Text Categorization.'' DTIC Document.

\hypertarget{ref-joachims1998text}{}
---------. 1998. ``Text Categorization with Support Vector Machines:
Learning with Many Relevant Features.'' In \emph{European Conference on
Machine Learning}, 137--42. Springer.

\hypertarget{ref-nallapati2016sequence}{}
Nallapati, Ramesh, Bing Xiang, and Bowen Zhou. 2016.
``Sequence-to-Sequence Rnns for Text Summarization.'' \emph{ArXiv
Preprint ArXiv:1602.06023}.

\hypertarget{ref-rehurek_lrec}{}
Rehurek, Radim, and Petr Sojka. 2010. ``Software Framework for Topic
Modelling with Large Corpora.'' In \emph{Proceedings of the LREC 2010
Workshop on New Challenges for NLP Frameworks}, 45--50. Valletta, Malta:
ELRA.

\hypertarget{ref-witschel2005using}{}
Witschel, Hans Friedrich. n.d. ``Using Decision Trees and Text Mining
Techniques for Extending Taxonomies.'' In. Citeseer.
